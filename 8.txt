Perfect â€” this is your **Machine Learning Assignment 3: Bank Customer Churn Prediction using Neural Network (ANN)**.
Letâ€™s break down **each step in plain, practical terms**, so you understand **whatâ€™s happening and why**.

---

## ðŸ§  Goal

Youâ€™re building an **Artificial Neural Network (ANN)** that predicts whether a **bank customer will leave the bank (churn)** or **stay**.

* `Exited = 1` â†’ Customer left the bank
* `Exited = 0` â†’ Customer stayed

---

## ðŸš€ Step-by-Step Explanation

---

### ðŸ§© **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from keras.models import Sequential
from keras.layers import Dense
```

**Whatâ€™s happening:**

* `pandas` â†’ to handle datasets (CSV files).
* `numpy` â†’ for mathematical operations.
* `matplotlib`, `seaborn` â†’ for plots and visualizations.
* `train_test_split` â†’ splits your data for training and testing.
* `StandardScaler` â†’ normalizes numerical data (important for neural networks).
* `Sequential`, `Dense` â†’ used to build the ANN model layer by layer.

---

### ðŸ“‚ **Step 2: Load Dataset**

```python
df = pd.read_csv("Churn_Modelling.csv")
df.head()
```

* Loads your CSV dataset into a pandas DataFrame.
* `.head()` shows the first 5 rows for a quick look at the data.

ðŸ§¾ Example columns:

```
RowNumber, CustomerId, Surname, CreditScore, Geography, Gender,
Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember,
EstimatedSalary, Exited
```

---

### ðŸ§± **Step 3: Basic Info Check**

```python
print(df.shape)
print(df.info())
print(df.isnull().sum())
```

* `.shape` â†’ shows number of rows and columns.
* `.info()` â†’ data types and memory usage.
* `.isnull().sum()` â†’ checks for missing (NaN) values in each column.

âœ… Ensures data is clean before modeling.

---

### ðŸŽ¯ **Step 4: Feature Selection**

```python
X = df.drop(columns=['RowNumber', 'CustomerId', 'Surname', 'Exited'])
y = df['Exited']
```

* Removes **irrelevant columns** (like ID, name â€” not useful for prediction).
* `X` â†’ features (inputs to the ANN).
* `y` â†’ target output (whether the customer left or not).

---

### ðŸ”¢ **Step 5: Encode Categorical Variables**

```python
X = pd.get_dummies(X, columns=['Geography', 'Gender'], drop_first=True)
```

* Converts **text columns** (e.g., *France*, *Spain*, *Germany*, *Male*, *Female*) into **numeric form** using one-hot encoding.
* `drop_first=True` avoids dummy variable trap (removes one column to prevent redundancy).

âœ… After this, all data in `X` is **numeric**, suitable for neural networks.

---

### âœ‚ï¸ **Step 6: Train-Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

* Splits data into:

  * `X_train`, `y_train` â†’ for training (80%)
  * `X_test`, `y_test` â†’ for testing (20%)
* `stratify=y` â†’ ensures both sets have similar proportions of churn (1s) and non-churn (0s).

---

### âš–ï¸ **Step 7: Feature Scaling**

```python
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```

* Neural networks perform better when all input features are **scaled** (mean = 0, std = 1).
* StandardScaler ensures that larger-valued columns (like salary) donâ€™t dominate the smaller ones (like age).

---

### ðŸ§  **Step 8: Build the ANN Model**

```python
model = Sequential()
model.add(Dense(16, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

**Explanation:**

* `Sequential()` â†’ means the ANN layers are stacked one after another.
* `Dense()` â†’ fully connected layers.

**Layers breakdown:**

| Layer            | Units | Purpose                  | Activation |
| ---------------- | ----- | ------------------------ | ---------- |
| Input + Hidden 1 | 16    | Detect patterns/features | ReLU       |
| Hidden 2         | 8     | Learn deeper patterns    | ReLU       |
| Output           | 1     | Predict churn (0 or 1)   | Sigmoid    |

**Compile step:**

* `optimizer='adam'` â†’ adjusts weights efficiently during training.
* `loss='binary_crossentropy'` â†’ ideal for binary classification.
* `metrics=['accuracy']` â†’ tracks performance.

---

### ðŸ‹ï¸ **Step 9: Train the Model**

```python
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)
```

* Trains the ANN for 50 cycles (**epochs**) on batches of 32 samples each.
* `validation_split=0.2` â†’ uses 20% of training data for validation.
* Tracks accuracy and loss during training.

---

### ðŸ“Š **Step 10: Evaluate the Model**

```python
y_pred = (model.predict(X_test) > 0.5)
acc = accuracy_score(y_test, y_pred)
print("âœ… Model Accuracy:", acc)
```

* The model outputs probabilities (0â€“1).
* `(>0.5)` converts them to class labels (0 or 1).
* Calculates **accuracy** using actual vs. predicted labels.

**Confusion Matrix:**

```python
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
```

* Shows performance visually:

  * Top-left: True Negatives
  * Bottom-right: True Positives
  * Others: Misclassifications

**Classification Report:**

```python
print(classification_report(y_test, y_pred))
```

* Gives **Precision**, **Recall**, **F1-score**.

---

### ðŸ“ˆ **Step 11: Plot Accuracy & Loss**

```python
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
```

* Shows how model performance evolved with each epoch.
* Ideally:

  * Training & validation accuracy increase.
  * Loss decreases.
* If validation accuracy drops while training accuracy rises â†’ **overfitting**.

---

### âœ… **Example Output**

```
Model Accuracy: 0.86
Precision: 0.82
Recall: 0.79
F1-score: 0.80
```

These mean:

* Model predicts customer churn correctly about **86%** of the time.
* Itâ€™s fairly balanced â€” not overfitting or underfitting.

---

## ðŸ“˜ Summary

| Step | Action              | Purpose                      |
| ---- | ------------------- | ---------------------------- |
| 1â€“3  | Load & inspect data | Understand dataset           |
| 4â€“5  | Clean and encode    | Prepare data for ML          |
| 6â€“7  | Split & scale       | Normalize inputs             |
| 8    | Build ANN           | Define model structure       |
| 9    | Train               | Learn patterns               |
| 10   | Evaluate            | Measure performance          |
| 11   | Visualize           | Understand learning behavior |

---

Would you like me to show a **visual diagram of this ANN (input â†’ hidden layers â†’ output)** so you can include it in your practical notebook report?
