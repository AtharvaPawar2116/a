ğŸ©º Assignment 4: Diabetes Prediction using KNN
ğŸ¯ Objective

Implement K-Nearest Neighbors (KNN) algorithm on the diabetes dataset, compute:

Confusion Matrix

Accuracy

Error Rate

Precision

Recall

ğŸ“Š Dataset

ğŸ“¥ Download from:
ğŸ‘‰ https://www.kaggle.com/datasets/abdallamahgoub/diabetes

Usually named: diabetes.csv

ğŸš€ Step-by-Step Jupyter Notebook Code
Step 1: Import Libraries
# Importing required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

Step 2: Load Dataset
# Load dataset
df = pd.read_csv("diabetes.csv")
df.head()

Step 3: Basic Information
print(df.shape)
print(df.info())
print(df.describe())
print(df.isnull().sum())


Output should show 768 rows Ã— 9 columns
No missing values â€” good for direct model training.

Step 4: Split Features and Target
X = df.drop('Outcome', axis=1)   # all features
y = df['Outcome']                # target (0 = No Diabetes, 1 = Diabetes)

Step 5: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

Step 6: Feature Scaling

KNN uses distance between points, so we scale all features equally.

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

Step 7: Train KNN Model
knn = KNeighborsClassifier(n_neighbors=5)  # you can test 3, 5, 7, etc.
knn.fit(X_train, y_train)

Step 8: Make Predictions
y_pred = knn.predict(X_test)

Step 9: Evaluate Model
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Evaluation Metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
err = 1 - acc

print(f"âœ… Accuracy: {acc:.3f}")
print(f"âŒ Error Rate: {err:.3f}")
print(f"ğŸ¯ Precision: {prec:.3f}")
print(f"ğŸ“ˆ Recall: {rec:.3f}")
print(f"ğŸ“Š F1 Score: {f1:.3f}")

Step 10: Try Different k Values
error_rates = []

for k in range(1, 15):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    pred_k = knn.predict(X_test)
    error = np.mean(pred_k != y_test)
    error_rates.append(error)

plt.figure(figsize=(8,4))
plt.plot(range(1,15), error_rates, marker='o', linestyle='dashed')
plt.title('Error Rate vs K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
plt.show()


Choose the k with minimum error (usually 5 or 7 works best).

ğŸ§¾ Example Output
âœ… Accuracy: 0.772
âŒ Error Rate: 0.228
ğŸ¯ Precision: 0.750
ğŸ“ˆ Recall: 0.710
ğŸ“Š F1 Score: 0.730
